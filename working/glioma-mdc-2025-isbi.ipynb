{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will cause jupyter to reload the *.py files we inport when we make changes to\n",
    "# them.  otherwiser you'd need to restar the server everytime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before importing anything we can setup the environment.  the imports will see these\n",
    "# options so it's a good place to set debug options and other environment variables\n",
    "# that we might want to control from the notebook.\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision opencv-python efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory for notebook\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make . the first search path for modules\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glioma\n",
    "import config\n",
    "import transform\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i've moved the config to config.py\n",
    "# you can make changes there if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup torch devices.  this means the code should run with or without a GPU\n",
    "compute_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "compute_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to do more with this !\n",
    "try:\n",
    "    torch.multiprocessing.set_start_method(\"spawn\")\n",
    "    print(\"spawned\")\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../output ../output/images ../output/models ../output/submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../input/glioma-mcd-2025/Data_122824\"\n",
    "TEST_ROOT = f\"{DATA_ROOT}/../../Oneshot_testingV2/Test-2.1\"\n",
    "TRAINING_ROOT = f\"{DATA_ROOT}/Glioma_MDC_2025_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glioma_training_data = glioma.CellTrainingDataset(TRAINING_ROOT)\n",
    "glioma_test_data = glioma.CellTestDataset(TEST_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example cell to show the transforms\n",
    "example_cell = glioma_training_data[0]\n",
    "example_cell.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a transform sequence with all the probabilities set to 1.0 to\n",
    "# demonstrate them.\n",
    "always_transforms = transform.TransformSequence(\n",
    "    [\n",
    "        transform.ResetCell(),\n",
    "        transform.RandomRotate(1.0),\n",
    "        transform.RandomHFlip(1.0),\n",
    "        transform.RandomVFlip(1.0),\n",
    "        transform.RandomXScale(1.0, config.MAX_CROP, config.MAX_PAD),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXScale(1.0, config.MAX_CROP, config.MAX_PAD),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXShift(1.0, config.MAX_LEFT_SHIFT, config.MIN_RIGHT_SHIFT),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXShift(1.0, config.MAX_LEFT_SHIFT, config.MIN_RIGHT_SHIFT),\n",
    "        transform.Turn(),\n",
    "        transform.CenterBoxCrop(1.0),\n",
    "        transform.RGBLevels(1.0),\n",
    "        transform.Saturation(1.0),\n",
    "        transform.Brightness(1.0),\n",
    "        transform.GammaContrast(1.0),\n",
    "        transform.CLAHE(1.0),\n",
    "        transform.Equalize(1.0),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_transforms = transform.TransformSequence(\n",
    "    [\n",
    "        transform.ResetCell(),\n",
    "        transform.Blur(1.0, 32, 0.5),\n",
    "        transform.CenterBoxCrop(1.0),\n",
    "    ]\n",
    ")\n",
    "# blur_example_cell = blur_transforms(example_cell, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsd_transforms = transform.TransformSequence(\n",
    "    [\n",
    "        transform.ResetCell(),\n",
    "        transform.CenterBoxCrop(1.0),\n",
    "        transform.HSD(1.0, 1.0),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the transforms\n",
    "# transformed_example_cell = always_transforms(example_cell, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the transforms\n",
    "# hsd_example_cell = hsd_transforms(example_cell, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_blur = transform.Blur(1.0, 0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transform.TransformSequence(\n",
    "    [\n",
    "        transform.ResetCell(),\n",
    "        training_blur,\n",
    "        transform.RandomRotate(0.5),\n",
    "        transform.RandomHFlip(0.2),\n",
    "        transform.RandomVFlip(0.2),\n",
    "        transform.RandomXScale(0.2, config.MAX_CROP, config.MAX_PAD),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXScale(0.2, config.MAX_CROP, config.MAX_PAD),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXShift(0.2, config.MAX_LEFT_SHIFT, config.MIN_RIGHT_SHIFT),\n",
    "        transform.Turn(),\n",
    "        transform.RandomXShift(0.2, config.MAX_LEFT_SHIFT, config.MIN_RIGHT_SHIFT),\n",
    "        transform.Turn(),\n",
    "        transform.CenterBoxCrop(1.0),\n",
    "        transform.HSD(0.5, 0.2),\n",
    "        transform.RGBLevels(0.2),\n",
    "        transform.Saturation(0.2),\n",
    "        transform.Brightness(0.2),\n",
    "        transform.GammaContrast(0.2),\n",
    "        transform.CLAHE(0.1),\n",
    "        transform.Equalize(0.01),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_blur = transform.Blur(1.0, 0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_blur = transform.Blur(1.0, 0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_transforms = transform.TransformSequence(\n",
    "    [\n",
    "        transform.ResetCell(),\n",
    "        validation_blur,\n",
    "        transform.CenterBoxCrop(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_transforms = transform.TransformSequence(\n",
    "#     [\n",
    "#         transform.ResetCell(),\n",
    "#         inference_blur,\n",
    "#         transform.CenterBoxCrop(1),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training, validation and test datasets\n",
    "training_dataset = glioma_training_data.training_split\n",
    "validation_dataset = glioma_training_data.validation_split\n",
    "test_dataset = glioma_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the collate function takes a list of cells (from the batch) and converts it to\n",
    "# a list of cells, and a tensor of Xs and ys\n",
    "def collate_fn(\n",
    "    batch,\n",
    "):\n",
    "    # batch is a list of cells\n",
    "    # extracts the Xs and ys\n",
    "    X = [cell.X for cell in batch]\n",
    "    y = [cell.y for cell in batch]\n",
    "    # convert to numpy arrays\n",
    "    X = np.stack(X)\n",
    "    y = np.asarray(y)\n",
    "    # format and shuffle the data dimensions\n",
    "    X = torch.from_numpy(X).float().to(compute_device)\n",
    "    y = torch.from_numpy(y).float().to(compute_device)\n",
    "    X = X.permute(0, 3, 1, 2)\n",
    "    return batch, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to do some final fixup on data fed to the model\n",
    "def data_fixup(\n",
    "    batch,\n",
    "):\n",
    "    cell, X, y = batch\n",
    "    # data should be in the range [0, 1], we want [-1, 1]\n",
    "    X = (X - 0.5) * 2.0\n",
    "    # targets are -1 and 1, we want 0 and 1\n",
    "    y = (y + 1.0) / 2.0\n",
    "    return cell, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline\n",
    "training_pipeline = training_dataset\n",
    "training_pipeline = training_pipeline.shuffle()\n",
    "training_pipeline = training_pipeline.map(training_transforms)\n",
    "training_pipeline = training_pipeline.batch(config.BATCH_SIZE)\n",
    "training_pipeline = training_pipeline.map(collate_fn)\n",
    "training_pipeline = training_pipeline.map(data_fixup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training validation pipeline\n",
    "training_validation_pipeline = training_dataset\n",
    "training_validation_pipeline = training_validation_pipeline.map(validation_transforms)\n",
    "training_validation_pipeline = training_validation_pipeline.batch(config.BATCH_SIZE)\n",
    "training_validation_pipeline = training_validation_pipeline.map(collate_fn)\n",
    "training_validation_pipeline = training_validation_pipeline.map(data_fixup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation pipeline\n",
    "validation_pipeline = validation_dataset\n",
    "validation_pipeline = validation_pipeline.map(validation_transforms)\n",
    "validation_pipeline = validation_pipeline.batch(config.BATCH_SIZE)\n",
    "validation_pipeline = validation_pipeline.map(collate_fn)\n",
    "validation_pipeline = validation_pipeline.map(data_fixup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pipeline\n",
    "test_pipeline = test_dataset\n",
    "test_pipeline = test_pipeline.map(validation_transforms)\n",
    "test_pipeline = test_pipeline.batch(config.BATCH_SIZE)\n",
    "test_pipeline = test_pipeline.map(collate_fn)\n",
    "test_pipeline = test_pipeline.map(data_fixup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_collate_fn(\n",
    "    batch,\n",
    "):\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loader = torch.utils.data.DataLoader(\n",
    "#     training_pipeline,\n",
    "#     batch_size=1,\n",
    "#     num_workers=config.NUM_WORKERS,\n",
    "#     # pin_memory=True,\n",
    "#     collate_fn=loader_collate_fn,\n",
    "#     persistent_workers=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_validation_loader = torch.utils.data.DataLoader(\n",
    "#     training_validation_pipeline,\n",
    "#     batch_size=1,\n",
    "#     num_workers=config.NUM_WORKERS,\n",
    "#     # pin_memory=True,\n",
    "#     collate_fn=loader_collate_fn,\n",
    "#     persistent_workers=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_loader = torch.utils.data.DataLoader(\n",
    "#     validation_pipeline,\n",
    "#     batch_size=1,\n",
    "#     num_workers=config.NUM_WORKERS,\n",
    "#     # pin_memory=True,\n",
    "#     collate_fn=loader_collate_fn,\n",
    "#     persistent_workers=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_pipeline,\n",
    "#     batch_size=1,\n",
    "#     num_workers=config.NUM_WORKERS,\n",
    "#     # pin_memory=True,\n",
    "#     collate_fn=loader_collate_fn,\n",
    "#     persistent_workers=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPV Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "glioma_model = model.HPV_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glioma_model.load_feature_extractor(\"model-000.state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with default weights\n",
    "glioma_model.load(\"model-001.state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glioma_model = glioma_model.to(compute_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should freeze these by default !\n",
    "glioma_model.freeze_features()\n",
    "# glioma_model.unfreeze_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model is giving up two outputs, these functions device how we interpret them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore one of the outputs, sigmoid on the other one\n",
    "def probabilities_from_model_1(\n",
    "    model_output,\n",
    "):\n",
    "    # print(type(model_output))\n",
    "    # first_output = model_output[:, 0]\n",
    "    probabilities = torch.sigmoid(model_output)\n",
    "    probabilities = probabilities.squeeze()\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference of the outputs is the class logits\n",
    "# def probabilities_from_model_2(\n",
    "#     model_output,\n",
    "# ):\n",
    "#     difference = model_output[:, 0] - model_output[:, 1]\n",
    "#     probabilities = torch.sigmoid(difference)\n",
    "\n",
    "#     return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something completely different\n",
    "# def probabilities_from_model_3(\n",
    "#     model_output,\n",
    "# ):\n",
    "#     value = model_output[:, 0] * model_output[:, 1]\n",
    "#     probabilities = torch.sigmoid(value)\n",
    "\n",
    "#     return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimiser_1(\n",
    "    model,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    amsgrad=config.AMS_GRAD,\n",
    "):\n",
    "    return torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        amsgrad=amsgrad,\n",
    "    )\n",
    "\n",
    "\n",
    "# optimiser_1 = make_optimiser_1(glioma_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_optimiser_2(\n",
    "#     model,\n",
    "#     learning_rate=config.LEARNING_RATE,\n",
    "#     weight_decay=config.WEIGHT_DECAY,\n",
    "#     momentum_decay=config.MOMENTUM_DECAY,\n",
    "# ):\n",
    "#     return torch.optim.NAdam(\n",
    "#         model.parameters(),\n",
    "#         lr=learning_rate,\n",
    "#         betas=(0.9, 0.999),\n",
    "#         eps=1e-08,\n",
    "#         weight_decay=weight_decay,\n",
    "#         momentum_decay=momentum_decay,\n",
    "#     )\n",
    "\n",
    "\n",
    "# optimiser_2 = make_optimiser_2(glioma_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_optimiser_3(\n",
    "#     model,\n",
    "#     learning_rate=config.LEARNING_RATE,\n",
    "#     momentum=config.SGD_MOMENTUM,\n",
    "#     weight_decay=config.WEIGHT_DECAY,\n",
    "# ):\n",
    "#     return torch.optim.SGD(\n",
    "#         model.parameters(),\n",
    "#         lr=learning_rate,\n",
    "#         momentum=momentum,\n",
    "#         weight_decay=weight_decay,\n",
    "#     )\n",
    "\n",
    "\n",
    "# optimiser_3 = make_optimiser_3(glioma_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for single classes,\n",
    "# the actual_class is the same as the actual_class_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy loss\n",
    "def loss_landscale_1(\n",
    "    predicted_class_probabilities: torch.Tensor,\n",
    "    actual_class_probabilities: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    loss = torch.nn.functional.binary_cross_entropy(\n",
    "        predicted_class_probabilities,\n",
    "        actual_class_probabilities,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard L2 loss\n",
    "def loss_landscale_2(\n",
    "    predicted_class_probabilities: torch.Tensor,\n",
    "    actual_class_probabilities: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        predicted_class_probabilities,\n",
    "        actual_class_probabilities,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard L1 loss\n",
    "def loss_landscale_3(\n",
    "    predicted_class_probabilities: torch.Tensor,\n",
    "    actual_class_probabilities: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    loss = torch.nn.functional.l1_loss(\n",
    "        predicted_class_probabilities,\n",
    "        actual_class_probabilities,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    predicted_class_probability,\n",
    "    actual_class,\n",
    "):\n",
    "    predicted_class = (predicted_class_probability >= 0.5).astype(int)\n",
    "    actual_class = actual_class.astype(int)\n",
    "\n",
    "    # print(\"Predicted Class Probability\")\n",
    "    # print(predicted_class_probability)\n",
    "    # print(\"Predicted Class\")\n",
    "    # print(predicted_class)\n",
    "    # print(\"Actual Class\")\n",
    "    # print(actual_class)\n",
    "\n",
    "    f1 = sklearn.metrics.f1_score(actual_class, predicted_class)\n",
    "    accuracy = sklearn.metrics.accuracy_score(actual_class, predicted_class)\n",
    "    precision = sklearn.metrics.precision_score(actual_class, predicted_class)\n",
    "    recall = sklearn.metrics.recall_score(actual_class, predicted_class)\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(actual_class, predicted_class_probability)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(\n",
    "    epoch,\n",
    "    model,\n",
    "    optimiser,\n",
    "    dataset,\n",
    "    loss_fn,\n",
    "    output_fn,\n",
    "):\n",
    "    # ensure model is in train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "    predicted_class_probabilities = []\n",
    "    actual_classes = []\n",
    "\n",
    "    batch_losses_1 = []\n",
    "    batch_losses_2 = []\n",
    "    batch_losses_3 = []\n",
    "\n",
    "    tqdm_iterator = tqdm(\n",
    "        enumerate(dataset),\n",
    "        total=len(dataset),\n",
    "        desc=f\"{epoch:04d} Training _.________ :\",\n",
    "    )\n",
    "    for i, (cell, X, actual_class) in tqdm_iterator:\n",
    "        # forward pass\n",
    "        optimiser.zero_grad()\n",
    "        model_output = model(X)\n",
    "        predicted_class_probability = output_fn(model_output)\n",
    "        batch_loss = loss_fn(predicted_class_probability, actual_class)\n",
    "\n",
    "        # calculate all three losses for comparison\n",
    "        loss_1 = loss_landscale_1(predicted_class_probability, actual_class)\n",
    "        loss_2 = loss_landscale_2(predicted_class_probability, actual_class)\n",
    "        loss_3 = loss_landscale_3(predicted_class_probability, actual_class)\n",
    "\n",
    "        # backward pass\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimiser.step()\n",
    "\n",
    "        # log losses and predictions\n",
    "        batch_losses.append(batch_loss)\n",
    "        predicted_class_probabilities.append(predicted_class_probability)\n",
    "        actual_classes.append(actual_class)\n",
    "        batch_losses_1.append(loss_1)\n",
    "        batch_losses_2.append(loss_2)\n",
    "        batch_losses_3.append(loss_3)\n",
    "\n",
    "        # save an example data batch\n",
    "        if i == 0:\n",
    "            torchvision.utils.save_image(\n",
    "                X.detach(),\n",
    "                f\"../output/images/sample_training_data_epoch_{epoch:04d}.png\",\n",
    "                normalize=True,\n",
    "            )\n",
    "\n",
    "        # update the progress bar\n",
    "        tqdm_iterator.set_description_str(\n",
    "            f\"{epoch:04d} Training {batch_loss.item():.8f} >\"\n",
    "        )\n",
    "\n",
    "    # stick all the predictons and ground truths together\n",
    "    predicted_class_probabilities = torch.cat(predicted_class_probabilities)\n",
    "    actual_classes = torch.cat(actual_classes)\n",
    "\n",
    "    # move them to the cpu\n",
    "    predicted_class_probabilities = predicted_class_probabilities.numpy(force=True)\n",
    "    actual_classes = actual_classes.numpy(force=True)\n",
    "\n",
    "    # print(\"TRAINING\")\n",
    "    # print(f\"{predicted_class_probability.shape=}\")\n",
    "    # print(f\"{actual_class.shape=}\")\n",
    "    # print(f\"{predicted_class_probabilities.shape=}\")\n",
    "    # print(f\"{actual_classes.shape=}\")\n",
    "\n",
    "    # use them to calculate the training metrics\n",
    "    metrics = calculate_metrics(predicted_class_probabilities, actual_classes)\n",
    "    metrics[\"loss\"] = torch.stack(batch_losses).mean().item()\n",
    "    metrics[\"loss_1\"] = torch.stack(batch_losses_1).mean().item()\n",
    "    metrics[\"loss_2\"] = torch.stack(batch_losses_2).mean().item()\n",
    "    metrics[\"loss_3\"] = torch.stack(batch_losses_3).mean().item()\n",
    "\n",
    "    tqdm_iterator.set_description_str(f\"{epoch:04d} Training {metrics[\"loss\"]:.8f} =\")\n",
    "    tqdm_iterator.refresh()\n",
    "    tqdm_iterator.close()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation(\n",
    "    epoch,\n",
    "    model,\n",
    "    dataset,\n",
    "    loss_fn,\n",
    "    output_fn,\n",
    "):\n",
    "    # ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    batch_losses = []\n",
    "    predicted_class_probabilities = []\n",
    "    actual_classes = []\n",
    "\n",
    "    batch_losses_1 = []\n",
    "    batch_losses_2 = []\n",
    "    batch_losses_3 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tqdm_iterator = tqdm(\n",
    "            enumerate(dataset),\n",
    "            total=len(dataset),\n",
    "            desc=f\"{epoch:04d} Validating _.________ :\",\n",
    "        )\n",
    "        for i, (_, X, actual_class) in tqdm_iterator:\n",
    "            # forward pass\n",
    "            model_output = model(X)\n",
    "            predicted_class_probability = output_fn(model_output)\n",
    "            batch_loss = loss_fn(predicted_class_probability, actual_class)\n",
    "\n",
    "            # calculate all three losses for comparison\n",
    "            loss_1 = loss_landscale_1(predicted_class_probability, actual_class)\n",
    "            loss_2 = loss_landscale_2(predicted_class_probability, actual_class)\n",
    "            loss_3 = loss_landscale_3(predicted_class_probability, actual_class)\n",
    "\n",
    "            # log losses and predictions\n",
    "            batch_losses.append(batch_loss)\n",
    "            predicted_class_probabilities.append(predicted_class_probability)\n",
    "            actual_classes.append(actual_class)\n",
    "            batch_losses_1.append(loss_1)\n",
    "            batch_losses_2.append(loss_2)\n",
    "            batch_losses_3.append(loss_3)\n",
    "\n",
    "            # save an example data batch\n",
    "            if epoch == 0 and i == 0:\n",
    "                torchvision.utils.save_image(\n",
    "                    X.detach(),\n",
    "                    f\"../output/images/sample_validation_data_epoch_{epoch:04d}.png\",\n",
    "                    normalize=True,\n",
    "                )\n",
    "\n",
    "            # update the progress bar\n",
    "            tqdm_iterator.set_description_str(\n",
    "                f\"{epoch:04d} Validating {batch_loss.item():.8f} >\"\n",
    "            )\n",
    "\n",
    "        # stick all the predictons and ground truths together\n",
    "        predicted_class_probabilities = torch.cat(predicted_class_probabilities)\n",
    "        actual_classes = torch.cat(actual_classes)\n",
    "\n",
    "        # move them to the cpu\n",
    "        predicted_class_probabilities = predicted_class_probabilities.numpy(force=True)\n",
    "        actual_classes = actual_classes.numpy(force=True)\n",
    "\n",
    "        # print(\"VALIDATION\")\n",
    "        # print(f\"{predicted_class_probability.shape=}\")\n",
    "        # print(f\"{actual_class.shape=}\")\n",
    "        # print(f\"{predicted_class_probabilities.shape=}\")\n",
    "        # print(f\"{actual_classes.shape=}\")\n",
    "\n",
    "        # use them to calculate the training metrics\n",
    "        metrics = calculate_metrics(predicted_class_probabilities, actual_classes)\n",
    "        metrics[\"loss\"] = torch.stack(batch_losses).mean().item()\n",
    "        metrics[\"loss_1\"] = torch.stack(batch_losses_1).mean().item()\n",
    "        metrics[\"loss_2\"] = torch.stack(batch_losses_2).mean().item()\n",
    "        metrics[\"loss_3\"] = torch.stack(batch_losses_3).mean().item()\n",
    "\n",
    "        tqdm_iterator.set_description_str(\n",
    "            f\"{epoch:04d} Validating {metrics[\"loss\"]:.8f} =\"\n",
    "        )\n",
    "        tqdm_iterator.refresh()\n",
    "        tqdm_iterator.close()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(\n",
    "    model,\n",
    "    dataset,\n",
    "    output_fn,\n",
    "    epoch=None,\n",
    "):\n",
    "    # ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # get the last submission file generated if there is one\n",
    "\n",
    "    last_predictions = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        prefix = \"\"\n",
    "        if epoch is not None:\n",
    "            prefix = f\"{epoch:04d} \"\n",
    "\n",
    "        for i, (cell, X, _) in tqdm(\n",
    "            enumerate(dataset),\n",
    "            total=len(dataset),\n",
    "            desc=f\"{prefix}Infering :\",\n",
    "        ):\n",
    "            # forward pass\n",
    "            model_output = model(X)\n",
    "            predicted_class_probability = output_fn(model_output)\n",
    "\n",
    "            predicted_class = predicted_class_probability.numpy(force=True) >= 0.5\n",
    "            predicted_class = predicted_class.astype(int)\n",
    "\n",
    "            for cell, prediction in zip(cell, predicted_class):\n",
    "                cell.mitosis = prediction\n",
    "                predictions.append(f\"{cell.image.label},{cell.label},{prediction}\")\n",
    "\n",
    "            # save an example data batch\n",
    "            if epoch == 0 and i == 0:\n",
    "                torchvision.utils.save_image(\n",
    "                    X.detach(),\n",
    "                    f\"../output/images/sample_inference_data_epoch_{epoch:04d}.png\",\n",
    "                    normalize=True,\n",
    "                )\n",
    "\n",
    "    predictions = sorted(predictions)\n",
    "\n",
    "    # compare to the last predictions\n",
    "\n",
    "    if epoch is not None and epoch > 0:\n",
    "\n",
    "        # load the last predictions\n",
    "        last_filename = f\"../output/submissions/epoch_{epoch-1:04d}.csv\"\n",
    "        with open(last_filename, \"r\") as csv_file:\n",
    "            last_predictions = csv_file.readlines()\n",
    "            # drop the header\n",
    "            last_predictions = last_predictions[1:]\n",
    "            last_predictions = [line.strip() for line in last_predictions]\n",
    "\n",
    "        # compare the predictions\n",
    "        mitotic = 0\n",
    "        same = 0\n",
    "        different = 0\n",
    "        for i, (last, current) in enumerate(zip(last_predictions, predictions)):\n",
    "            if last == f\"{i+1},{current}\":\n",
    "                same += 1\n",
    "            else:\n",
    "                different += 1\n",
    "            if current.split(\",\")[2] == \"1\":\n",
    "                mitotic += 1\n",
    "        print(\n",
    "            f\"{epoch-1:04d} Inference has {same} same and {different} different predictions - {mitotic} mitotic.\"\n",
    "        )\n",
    "\n",
    "    # write out the results\n",
    "\n",
    "    filename = f\"../output/submission.csv\"\n",
    "    if epoch is not None:\n",
    "        filename = f\"../output/submissions/epoch_{epoch:04d}.csv\"\n",
    "\n",
    "    with open(filename, \"w\") as csv_file:\n",
    "        # write the header\n",
    "        csv_file.write(\"Row ID,Image ID,Label ID,Prediction\\n\")\n",
    "        # write the predictions\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            csv_file.write(f\"{i+1},{prediction}\\n\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGS\n",
    "training_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the training logs if they have been saved so we can continue training\n",
    "training_log = None\n",
    "try:\n",
    "    training_log = pd.read_csv(config.TRAINING_LOG)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "if training_log is None:\n",
    "    training_log = []\n",
    "else:\n",
    "    # training_log = training_log.drop(columns=\"Unnamed: 0\")\n",
    "    # convert back to a list\n",
    "    training_log = training_log.to_dict(\"records\")\n",
    "    # convert the values to a list [epoch, subset, metric, value]]\n",
    "    training_log = [list(row.values()) for row in training_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tells us the last completed epoch by checking the training log\n",
    "def last_completed_epoch():\n",
    "    completed_epochs = [row[0] for row in training_log]\n",
    "    if len(completed_epochs) == 0:\n",
    "        return -1\n",
    "    return max(completed_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we can reload the model from saved state from training\n",
    "def reload_latest_model():\n",
    "    last_epoch = last_completed_epoch()\n",
    "    if last_epoch == -1:\n",
    "        return\n",
    "    glioma_model.load(f\"../output/models/{last_epoch:04d}.state\")\n",
    "\n",
    "\n",
    "reload_latest_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_metric(\n",
    "    metric: str,\n",
    "    metrics: pd.DataFrame,\n",
    "):\n",
    "    metric_df = metrics[metrics[\"metric\"] == metric]\n",
    "    ax = sns.lineplot(\n",
    "        data=metric_df,\n",
    "        x=\"epoch\",\n",
    "        y=\"value\",\n",
    "        hue=\"subset\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/{metric}.png\", dpi=300)\n",
    "    plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_metric(\n",
    "    metric: str,\n",
    "    metrics: pd.DataFrame,\n",
    "):\n",
    "    metric_df = metrics[metrics[\"metric\"] == metric]\n",
    "    ax = sns.lineplot(\n",
    "        data=metric_df,\n",
    "        x=\"epoch\",\n",
    "        y=\"value\",\n",
    "        hue=\"subset\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/{metric}.png\", dpi=300)\n",
    "    # create a log version\n",
    "    ax.set_yscale(\"log\")\n",
    "    # figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/{metric}_log.png\", dpi=300)\n",
    "    plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_loss_metric(\n",
    "    metrics: pd.DataFrame,\n",
    "):\n",
    "    metric_df = metrics[metrics[\"metric\"].str.startswith(\"loss\")]\n",
    "    ax = sns.lineplot(\n",
    "        data=metric_df,\n",
    "        x=\"epoch\",\n",
    "        y=\"value\",\n",
    "        hue=\"subset\",\n",
    "        style=\"metric\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/all_loss.png\", dpi=300)\n",
    "    # create a log version\n",
    "    ax.set_yscale(\"log\")\n",
    "    # figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/all_loss_log.png\", dpi=300)\n",
    "    # start from epoch 400\n",
    "    plt.xlim(left=400)\n",
    "    figure.savefig(f\"../output/all_loss_log_400.png\", dpi=300)\n",
    "    plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_near_one_metric(\n",
    "    metric: str,\n",
    "    metrics: pd.DataFrame,\n",
    "):\n",
    "    metric_df = metrics[metrics[\"metric\"] == metric]\n",
    "    ax = sns.lineplot(\n",
    "        data=metric_df,\n",
    "        x=\"epoch\",\n",
    "        y=\"value\",\n",
    "        hue=\"subset\",\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/{metric}.png\", dpi=300)\n",
    "    # create a zoomed in version near 1\n",
    "    ax.set_ylim(0.9, 1.0)\n",
    "    # figure = ax.figure\n",
    "    # figure.legend()\n",
    "    figure.savefig(f\"../output/{metric}_zoom.png\", dpi=300)\n",
    "    metric_df = metrics[metrics[\"metric\"] == metric]\n",
    "    current_epoch = last_completed_epoch()\n",
    "    start_epoch = current_epoch - 10\n",
    "    if start_epoch < 0:\n",
    "        start_epoch = 0\n",
    "    min_value = metric_df[metric_df[\"epoch\"] >= start_epoch][\"value\"].min()\n",
    "    max_value = metric_df[metric_df[\"epoch\"] >= start_epoch][\"value\"].max()\n",
    "    ax.set_ylim(min_value, max_value)\n",
    "    figure.savefig(f\"../output/{metric}_zoom_auto.png\", dpi=300)\n",
    "    plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots(\n",
    "    df: pd.DataFrame,\n",
    "):\n",
    "\n",
    "    plot_all_loss_metric(df)\n",
    "\n",
    "    plot_loss_metric(\"loss\", df)\n",
    "    plot_loss_metric(\"loss_1\", df)\n",
    "    plot_loss_metric(\"loss_2\", df)\n",
    "    plot_loss_metric(\"loss_3\", df)\n",
    "\n",
    "    plot_near_one_metric(\"f1\", df)\n",
    "    plot_near_one_metric(\"accuracy\", df)\n",
    "    plot_near_one_metric(\"precision\", df)\n",
    "    plot_near_one_metric(\"recall\", df)\n",
    "    plot_near_one_metric(\"roc_auc\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimiser,\n",
    "    loss_fn,\n",
    "    output_fn,\n",
    "    epochs=config.EPOCHS,\n",
    "):\n",
    "\n",
    "    first_epoch = last_completed_epoch() + 1\n",
    "    last_epoch = first_epoch + epochs\n",
    "\n",
    "    for epoch in range(first_epoch, last_epoch):\n",
    "\n",
    "        # train the model\n",
    "        training_metrics = do_training(\n",
    "            epoch,\n",
    "            model,\n",
    "            optimiser,\n",
    "            training_pipeline,  # training_validation_pipeline,\n",
    "            loss_fn,\n",
    "            output_fn,\n",
    "        )\n",
    "\n",
    "        # train the model\n",
    "        training_metrics = do_training(\n",
    "            epoch,\n",
    "            model,\n",
    "            optimiser,\n",
    "            training_validation_pipeline,\n",
    "            loss_fn,\n",
    "            output_fn,\n",
    "        )\n",
    "\n",
    "        # really we should be doing this to get consistent metrics for\n",
    "        # the training dataset\n",
    "        training_dataset_metrics = do_validation(\n",
    "            epoch,\n",
    "            model,\n",
    "            training_validation_pipeline,\n",
    "            loss_fn,\n",
    "            output_fn,\n",
    "        )\n",
    "\n",
    "        validation_dataset_metrics = do_validation(\n",
    "            epoch,\n",
    "            model,\n",
    "            validation_pipeline,\n",
    "            loss_fn,\n",
    "            output_fn,\n",
    "        )\n",
    "\n",
    "        do_inference(\n",
    "            model,\n",
    "            test_pipeline,\n",
    "            output_fn,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        # record all the metrics in the training log\n",
    "\n",
    "        for metric_name, metric_value in training_metrics.items():\n",
    "            training_log.append([epoch, \"training\", metric_name, metric_value])\n",
    "\n",
    "        for metric_name, metric_value in training_dataset_metrics.items():\n",
    "            training_log.append([epoch, \"training_dataset\", metric_name, metric_value])\n",
    "\n",
    "        for metric_name, metric_value in validation_dataset_metrics.items():\n",
    "            training_log.append(\n",
    "                [epoch, \"validation_dataset\", metric_name, metric_value]\n",
    "            )\n",
    "\n",
    "        # save the training log and model state\n",
    "\n",
    "        df = pd.DataFrame(training_log, columns=[\"epoch\", \"subset\", \"metric\", \"value\"])\n",
    "        df.to_csv(config.TRAINING_LOG, index=False)\n",
    "\n",
    "        model.save(f\"../output/models/{epoch:04d}.state\")\n",
    "\n",
    "        # update the graphs from the training log\n",
    "        update_plots(df)\n",
    "\n",
    "        # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_inference(\n",
    "    glioma_model,\n",
    "    test_pipeline,\n",
    "    probabilities_from_model_1,\n",
    "    epoch=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_validation(\n",
    "    0,\n",
    "    glioma_model,\n",
    "    validation_pipeline,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glioma_model.freeze_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run with optimiser_1, loss_landscale_1, very small learning rate\n",
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.001)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run with optimiser_1, loss_landscale_1\n",
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0005)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run with optimiser_1, loss_landscale_1\n",
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0001)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.00005)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0001)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0001)\n",
    "for i in range(1, 11):\n",
    "    training_blur.fade = i / 10\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.00005)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0001)\n",
    "training_blur.fade = 1.0\n",
    "for i in reversed([0, 1, 2, 4, 8, 16]):\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.0001)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscale_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.00005)\n",
    "train(\n",
    "    glioma_model,\n",
    "    optimiser_1,\n",
    "    loss_landscalpe_1,\n",
    "    probabilities_from_model_1,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 2, 4, 8, 16, 32]:\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed([1, 2, 4, 6, 8, 10, 12, 16]):\n",
    "    validation_blur.kernel_size = i\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed([1, 2, 4, 6, 8, 10, 12, 14, 16]):\n",
    "    validation_blur.kernel_size = i\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_1 = make_optimiser_1(glioma_model, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glioma_model.unfreeze_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed([1, 2, 4, 6, 8, 10, 12, 14, 16]):\n",
    "    validation_blur.kernel_size = i\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed([1]):\n",
    "    validation_blur.kernel_size = i\n",
    "    training_blur.kernel_size = i\n",
    "    train(\n",
    "        glioma_model,\n",
    "        optimiser_1,\n",
    "        loss_landscale_1,\n",
    "        probabilities_from_model_1,\n",
    "        epochs=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
